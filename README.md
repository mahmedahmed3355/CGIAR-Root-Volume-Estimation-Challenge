# ğŸŒ± CGIAR Root Volume Estimation Challenge

### ğŸ§  Overview

This project was developed as part of the **CGIAR Root Volume Estimation Challenge**, which aims to accurately estimate the **root volume** of plants from images using advanced machine learning and computer vision techniques.
The task involves building a regression model capable of predicting the continuous target variable â€” *root volume* â€” from multi-modal data sources, including image-based and tabular features.

---

## ğŸš€ Project Highlights

* **Competition:** [CGIAR Root Volume Estimation Challenge](https://www.zindi.jp/competitions/)
* **Task Type:** Regression
* **Frameworks Used:** PyTorch, LightGBM, CatBoost, XGBoost, scikit-learn, timm
* **Public Score:** 1.06
* **Private Score:** 1.38
* **Author:** Mohamed Ahmed (`mohamed2236945`)

---

## ğŸ“‚ Project Structure

```
CGIAR-Root-Volume-Estimation/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/                # Training images
â”‚   â”œâ”€â”€ test/                 # Test images
â”‚   â”œâ”€â”€ train.csv             # Metadata and root volume labels
â”‚   â””â”€â”€ sample_submission.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py         # Image and feature preprocessing
â”‚   â”œâ”€â”€ dataset.py            # Custom PyTorch dataset class
â”‚   â”œâ”€â”€ feature_engineering.py# Feature generation and augmentation
â”‚   â”œâ”€â”€ model_lightgbm.py     # LightGBM training script
â”‚   â”œâ”€â”€ model_catboost.py     # CatBoost training script
â”‚   â”œâ”€â”€ model_xgb.py          # XGBoost training script
â”‚   â”œâ”€â”€ model_mlp.py          # Simple PyTorch MLP (optional)
â”‚   â””â”€â”€ utils.py              # Helper functions
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb     # EDA and visualization notebook
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ submissions/          # Final submission CSV files
â”‚   â””â”€â”€ models/               # Saved trained models (.pkl / .bin)
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ main.py                   # Main training & inference pipeline
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/mahmedahmed3355/CGIAR-Root-Volume-Estimation.git
cd CGIAR-Root-Volume-Estimation
pip install -r requirements.txt
```

### Requirements

* Python 3.10+
* PyTorch
* timm
* scikit-learn
* xgboost
* lightgbm
* catboost
* Pillow
* numpy, pandas, matplotlib, seaborn

---

## ğŸ§© Approach

### 1. **Data Preprocessing**

* Converted and resized all input images for consistent shape and normalization.
* Augmented images using standard geometric transformations (rotation, flip, color jitter).
* Extracted **image embeddings** using pretrained `timm` models (e.g. EfficientNet, ResNet).
* Combined image features with **tabular metadata** to form a hybrid feature vector.

### 2. **Feature Engineering**

* Statistical aggregations and normalization of numerical features.
* Polynomial feature combinations for selected predictors.
* Added domain-specific ratios and interaction terms.
* Applied PCA for dimensionality reduction and noise filtering.

### 3. **Modeling**

Trained multiple regressors:

* ğŸŸ¢ **CatBoostRegressor**
* ğŸ”µ **LightGBMRegressor**
* ğŸ”´ **XGBoostRegressor**
* (Optional) PyTorch **MLPRegressor**

Each model was fine-tuned using Bayesian optimization or grid search on validation folds.

### 4. **Ensembling**

* Averaged predictions from top-performing models using weighted blending:

  ```python
  final_pred = 0.4 * catboost_pred + 0.35 * lgb_pred + 0.25 * xgb_pred
  ```
* Ensemble reduced overfitting and improved generalization on the private leaderboard.

---

## ğŸ“ˆ Results

| Model                | Public Score | Private Score |
| -------------------- | ------------ | ------------- |
| LightGBM             | 1.09         | 1.42          |
| CatBoost             | 1.07         | 1.40          |
| XGBoost              | 1.08         | 1.41          |
| **Ensemble (Final)** | **1.06**     | **1.38**      |

---

## ğŸ’¾ Inference

```bash
python main.py --mode inference --model ensemble
```

This script loads trained models and generates a submission file:

```
submissions/final_submission.csv
```

---

## ğŸ§° Key Learnings

* Hybrid feature fusion between image embeddings and tabular data can greatly improve accuracy.
* Ensemble averaging across gradient boosting models is highly effective for small data challenges.
* Proper validation and cross-checking prevent leaderboard overfitting.

---

## ğŸ“œ License

This project is released under the **MIT License**.

---

## ğŸ‘¤ Author

**Mohamed Ahmed**
AI Engineer | ML / CV / LLM Specialist
ğŸ“§ [engmohamedelshrbeny@gmail.com](mailto:engmohamedelshrbeny@gmail.com)
ğŸŒ [LinkedIn](https://linkedin.com/in/engmohamedelshrbeny) | [GitHub](https://github.com/engmohamedelshrbeny)
